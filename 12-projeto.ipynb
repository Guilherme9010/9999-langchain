{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Projeto - Converse com seus Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "caminhos = [\n",
    "    \"files/apostila.pdf\",\n",
    "    \"files/LLM.pdf\",\n",
    "    ]\n",
    "\n",
    "paginas = []\n",
    "for caminho in caminhos:\n",
    "    loader = PyPDFLoader(caminho)\n",
    "    paginas.extend(loader.load())\n",
    "\n",
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(paginas)\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata['source'] = doc.metadata['source'].replace('arquivos/', '')\n",
    "    doc.metadata['doc_id'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio = 'arquivos/chat_retrieval_db'\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings_model,\n",
    "    persist_directory=diretorio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'O que é Hugging Face e como faço para acessá-lo?',\n",
       " 'result': 'Hugging Face é uma comunidade e plataforma que reúne centenas de milhares de modelos de aprendizado de máquina, especialmente focados em processamento de linguagem natural (NLP). Os modelos disponíveis podem ser usados para uma variedade de tarefas, como geração de texto, resumo e classificação. A comunidade de código aberto da Hugging Face está crescendo rapidamente e busca alcançar o desempenho de modelos proprietários.\\n\\nPara acessar o Hugging Face, você pode visitar o site oficial da plataforma. Lá, você encontrará uma vasta biblioteca de modelos, tutoriais e documentação para ajudá-lo a começar a usar diferentes modelos para seus projetos. Além disso, você pode explorar mapas de comunidade, fóruns e outros recursos para aprofundar seu conhecimento.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pergunta = \"O que é Hugging Face e como faço para acessá-lo?\"\n",
    "chat_chain.invoke({\"query\": pergunta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnica de Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chain_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Utilize o contexto fornecido para responder a pergunta ao final. \n",
    "Se você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\n",
    "Utilize três frases no máximo, mantenha a resposta concisa.\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
    "    chain_type_kwargs={\"prompt\":chain_prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não sei.\n"
     ]
    }
   ],
   "source": [
    "pergunta = 'O que é Veleiro Katamarã ?'\n",
    "#pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "print(resposta['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': 'lucas', 'total_pages': 28, 'page': 3, 'creationdate': '2016-05-04T10:06:39-03:00', 'source': 'files/apostila.pdf', 'moddate': '2016-05-04T10:06:39-03:00', 'producer': 'Microsoft® Word 2013', 'creator': 'Microsoft® Word 2013', 'page_label': '4', 'doc_id': 9}, page_content='Reforçando, este é um material introdutório. Tem muito mais para aprender em Python: \\norientação a objetos, programação funcional, metaprogramação, interface gráfica, \\nexpressões regulares, threads, tratamento de exceções, funções anônimas, geradores, \\ndesenvolvimento web, aplicativos móveis, entre outras.  \\nBem-vindo ao mundo Python! \\n \\n \\n \\nProf. Dr. João Luiz Franco \\nTutor do grupo PET - ADS / São Carlos'),\n",
       " Document(metadata={'doc_id': 51, 'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext', 'creationdate': \"D:20231017132635Z00'00'\", 'page_label': '1', 'page': 0, 'total_pages': 9, 'creator': 'PyPDF', 'moddate': \"D:20231017132635Z00'00'\", 'source': 'files/LLM.pdf'}, page_content='E-BOOK \\nUm guia compacto sobre Large Language Models (LLM)'),\n",
       " Document(metadata={'creator': 'PyPDF', 'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext', 'total_pages': 9, 'page_label': '6', 'doc_id': 69, 'source': 'files/LLM.pdf', 'page': 5, 'creationdate': \"D:20231017132635Z00'00'\", 'moddate': \"D:20231017132635Z00'00'\"}, page_content='.5, GPT-4 e outros) e geralmente obter uma resposta rápida. Eles estão entre os modelos de maior desempenho, treinados em conjuntos de dados enormes, e são capazes de realizar tarefas extremamente complexas tanto do ponto de vista técnico, como geração de código, quanto do ponto de vista criativo, como escrever poesia em um estilo específico. A desvantagem desses serviços é a quantidade absolutamente enorme de recursos computacionais necessários não apenas para treiná-los (a OpenAI afirmou que o GPT-4 custou mais de US$ 100 milhões para desenvolver), mas também para fornecer as respostas. Por esse motivo, esses modelos extremamente grandes provavelmente sempre estarão sob o controle de organizações'),\n",
       " Document(metadata={'creator': 'PyPDF', 'page': 7, 'moddate': \"D:20231017132635Z00'00'\", 'page_label': '8', 'creationdate': \"D:20231017132635Z00'00'\", 'source': 'files/LLM.pdf', 'total_pages': 9, 'doc_id': 76, 'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext'}, page_content='8  \\n \\n     PARTE 4')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug do LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"O que é Veleiro Katamarã ?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"O que é Veleiro Katamarã ?\",\n",
      "  \"context\": \"Reforçando, este é um material introdutório. Tem muito mais para aprender em Python: \\norientação a objetos, programação funcional, metaprogramação, interface gráfica, \\nexpressões regulares, threads, tratamento de exceções, funções anônimas, geradores, \\ndesenvolvimento web, aplicativos móveis, entre outras.  \\nBem-vindo ao mundo Python! \\n \\n \\n \\nProf. Dr. João Luiz Franco \\nTutor do grupo PET - ADS / São Carlos\\n\\nE-BOOK \\nUm guia compacto sobre Large Language Models (LLM)\\n\\n.5, GPT-4 e outros) e geralmente obter uma resposta rápida. Eles estão entre os modelos de maior desempenho, treinados em conjuntos de dados enormes, e são capazes de realizar tarefas extremamente complexas tanto do ponto de vista técnico, como geração de código, quanto do ponto de vista criativo, como escrever poesia em um estilo específico. A desvantagem desses serviços é a quantidade absolutamente enorme de recursos computacionais necessários não apenas para treiná-los (a OpenAI afirmou que o GPT-4 custou mais de US$ 100 milhões para desenvolver), mas também para fornecer as respostas. Por esse motivo, esses modelos extremamente grandes provavelmente sempre estarão sob o controle de organizações\\n\\n8  \\n \\n     PARTE 4\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Utilize o contexto fornecido para responder a pergunta ao final. \\nSe você não sabe a resposta, apenas diga que não sabe e não invente uma resposta.\\nUtilize três frases no máximo, mantenha a resposta concisa.\\n\\nContexto: Reforçando, este é um material introdutório. Tem muito mais para aprender em Python: \\norientação a objetos, programação funcional, metaprogramação, interface gráfica, \\nexpressões regulares, threads, tratamento de exceções, funções anônimas, geradores, \\ndesenvolvimento web, aplicativos móveis, entre outras.  \\nBem-vindo ao mundo Python! \\n \\n \\n \\nProf. Dr. João Luiz Franco \\nTutor do grupo PET - ADS / São Carlos\\n\\nE-BOOK \\nUm guia compacto sobre Large Language Models (LLM)\\n\\n.5, GPT-4 e outros) e geralmente obter uma resposta rápida. Eles estão entre os modelos de maior desempenho, treinados em conjuntos de dados enormes, e são capazes de realizar tarefas extremamente complexas tanto do ponto de vista técnico, como geração de código, quanto do ponto de vista criativo, como escrever poesia em um estilo específico. A desvantagem desses serviços é a quantidade absolutamente enorme de recursos computacionais necessários não apenas para treiná-los (a OpenAI afirmou que o GPT-4 custou mais de US$ 100 milhões para desenvolver), mas também para fornecer as respostas. Por esse motivo, esses modelos extremamente grandes provavelmente sempre estarão sob o controle de organizações\\n\\n8  \\n \\n     PARTE 4\\n\\nPergunta: O que é Veleiro Katamarã ?\\n\\nResposta:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] [552ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Não sei.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Não sei.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 3,\n",
      "                \"prompt_tokens\": 332,\n",
      "                \"total_tokens\": 335,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_e665f7564b\",\n",
      "              \"id\": \"chatcmpl-CBef9EujjTr9T5r79qEfZFvN6Dic2\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--85b05b06-6e98-46d8-a599-827aef9b6542-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 332,\n",
      "              \"output_tokens\": 3,\n",
      "              \"total_tokens\": 335,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 3,\n",
      "      \"prompt_tokens\": 332,\n",
      "      \"total_tokens\": 335,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_e665f7564b\",\n",
      "    \"id\": \"chatcmpl-CBef9EujjTr9T5r79qEfZFvN6Dic2\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] [553ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Não sei.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] [553ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Não sei.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA] [862ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "#pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "pergunta = 'O que é Veleiro Katamarã ?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinamento da Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face é uma comunidade e plataforma que fornece acesso a uma ampla gama de modelos de aprendizado de máquina, especialmente voltados para o processamento de linguagem natural (NLP). Esses modelos são extremamente úteis para diversas aplicações, como geração de texto, resumo, tradução, entre outros. A Hugging Face se destaca por sua contribuição ao código aberto e por permitir que desenvolvedores e pesquisadores compartilhem e utilizem modelos colaborativamente.\n",
      "\n",
      "Recentemente, um dos maiores saltos de desempenho dos modelos da Hugging Face veio da integração do feedback humano diretamente no processo de treinamento. Isso significa que os modelos podem ser refinados com base nas interações e preferências dos usuários, resultando em um desempenho mais alinhado às expectativas e necessidades reais.\n",
      "\n",
      "Aqui estão alguns passos para começar a usar Hugging Face:\n",
      "\n",
      "1. **Visitar o site**: Acesse [huggingface.co](https://huggingface.co), onde você encontrará a biblioteca de modelos.\n",
      "\n",
      "2. **Criar uma conta**: Você pode criar uma conta gratuita para acessar recursos adicionais e participar da comunidade.\n",
      "\n",
      "3. **Explorar modelos**: No site, você poderá navegar por uma vasta coleção de modelos prontos para uso. Cada modelo pode ser usado diretamente ou adaptado para suas necessidades.\n",
      "\n",
      "4. **Recursos educativos**: Para quem deseja aprender mais sobre o uso de LLMs (Modelos de Linguagem de Grande Escala), recomenda-se assistir a apresentações e tutoriais. A plataforma oferece diversos materiais educativos que abrangem desde a criação de modelos até casos de uso práticos.\n",
      "\n",
      "5. **Utilizar a API**: Você pode integrar facilmente os modelos em suas aplicações utilizando a API do Hugging Face, que é bem documentada e de fácil uso.\n",
      "\n",
      "6. **Consultar a documentação**: A documentação disponível no site é uma excelente fonte de informações sobre como usar os modelos, bem como sobre as bibliotecas relacionadas, como o Transformers e o Datasets.\n",
      "\n",
      "Assim, com a integração do feedback humano no treinamento, seus modelos poderão se tornar ainda mais eficazes e personalizados. Você será bem-vindo ao mundo Python, e ao mesmo tempo, poderá explorar os recursos valiosos que a Hugging Face tem a oferecer!\n"
     ]
    }
   ],
   "source": [
    "chat_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type='mmr'),\n",
    "    chain_type='refine'\n",
    ")\n",
    "\n",
    "pergunta = 'O que é Hugging Face e como faço para acessá-lo?'\n",
    "resposta = chat_chain.invoke({'query': pergunta})\n",
    "print(resposta['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
